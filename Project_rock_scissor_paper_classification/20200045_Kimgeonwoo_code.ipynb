{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "374acf0a8600279850f148506a6b4762afb2721aa53d2847250d0f3e4a044f03"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zv8rwJKRSWM"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 코드로 형식 지정됨\n",
        "```\n",
        "\n",
        "# 2023 Fall EE331 Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946wVZ9hRFLG"
      },
      "source": [
        "**OBJECTIVE**\n",
        "\n",
        "You are provided with a simple PyTorch model that classfies images into one of three categories: rock, paper or scissors. The current model uses just one fully connected (FC) layer for classfication. Your task is to do the following:\n",
        "\n",
        "1. Model Design\n",
        "\n",
        "\n",
        "> Replace the single FC layer in the '' class with a neural network architecture of your choice. Consider using combinations of layers such as multiple FC layers, convolutional layers, activation functions and dropout layers.\n",
        "\n",
        "\n",
        "2. Training\n",
        "\n",
        "\n",
        "> Once the model is designed,\n",
        "\n",
        "use the providced training loop to train the model. Adjust the hyperparameters if necessary. You may change the provided dataset with a dataset of your choice. However, this must be stated in the report you are going to write.\n",
        "\n",
        "\n",
        "You are free to change the code sections 1~????.\n",
        "\n",
        "For more detail on the report, refer to the project pdf file uploaded in KLMS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Libraries"
      ],
      "metadata": {
        "id": "59Cr79j25X-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing_extensions import dataclass_transform\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torch.nn.functional as F\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# !pip install -Uqq ipdb\n",
        "# import ipdb\n",
        "# %pdb on\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "VPrFX46oK-mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38869b3-0dc7-49eb-c605-088447444f5a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "J3zVZmPqfRIf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp-V10BFRDar"
      },
      "source": [
        "**1. DATASET**\n",
        "\n",
        "The following link has some rock scissors paper dataset you can use:\n",
        "\n",
        "https://kaistackr-my.sharepoint.com/:f:/g/personal/shimjay17_kaist_ac_kr/EkBKx6r7RU1BnsFc5QbqFBMBHhOzF8SFoHXzaJAr1n7a3g?e=ElWBrF\n",
        "\n",
        "To use this dataset, you will have to upload these datasets to your google drive and specify its location in the dataset path in the code below.\n",
        "\n",
        "However, though it is not a necessity, we recommend that you test your model on additional datasets that are available to use. Remember that your model is going to be graded on an unseen custom dataset and will require your model good generalisation.\n",
        "\n",
        "If you do decide to use an additional dataset, you may need to edit the 'loadLabels' definition accordingly in the 'Dataset' class."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######### Declare dataset path below! #########\n",
        "#DATASET_PATH = '/content/drive/My Drive/archive/rsp'\n",
        "DATASET_PATH1 = '/content/drive/MyDrive/MLintro/data_achive/archive 1/rps-cv-images'\n",
        "DATASET_PATH2 = '/content/drive/MyDrive/MLintro/data_achive/archive 2'\n",
        "DATASET_PATH3 = '/content/drive/MyDrive/MLintro/data_achive/mydata'\n",
        "###############################################\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, dataset_dir, transform=None, target_transform=None):\n",
        "        self.class_list = {'rock': 0, 'scissors': 1, 'paper': 2}\n",
        "        self.labels_map = {0:'rock', 1:'scissors', 2:'paper'}\n",
        "        self.path = dataset_dir #DATASET_PATH\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(self.path, '**', '*.jpg'), recursive=True)) + sorted(glob.glob(os.path.join(self.path, '**', '*.png'), recursive=True))\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()]) if transform is None else transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = self.loadImage(img_path)\n",
        "        image = cv2.resize(image, (28, 28))\n",
        "\n",
        "        # Apply the transform to the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.loadLabels(img_path)\n",
        "        return image, torch.tensor(label, dtype=torch.long)  # Ensure label is a long tensor\n",
        "\n",
        "    def loadImage(self, path):\n",
        "        img = cv2.imread(path)\n",
        "        img = img.astype(np.float32)/255.0  # 이미지\n",
        "        return img\n",
        "\n",
        "    def loadLabels(self, image_paths):\n",
        "        class_name = os.path.dirname(image_paths).split('/')[-1]\n",
        "        return self.class_list[class_name]"
      ],
      "metadata": {
        "id": "ZMgYYN9zLVtM"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPgmOpOzRMrm"
      },
      "source": [
        "**2. Classification Code**\n",
        "\n",
        "In this section, you are required to implement a network of your choice to get better performance.\n",
        "You are free to edit this skeleton code however you want."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_BN2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN_BN2, self).__init__()\n",
        "    # Fill in the blank\n",
        "    ##################################################\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(5,5))\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Max-pooling with kernel size = 2×2, stride = 2\n",
        "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=5)\n",
        "    self.bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "    #####################################################\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=256, out_features=64)\n",
        "    self.bn3 = nn.BatchNorm1d(64)\n",
        "    self.fc2 = nn.Linear(in_features=64, out_features=3) # The output size of last FC layer must be equal to num of class\n",
        "\n",
        "  def forward(self, x):\n",
        "    batchsize = x.size(0)\n",
        "    x = self.pool(F.relu(self.bn1(self.conv1(x)))) # Convolutional layer -> BN -> ReLU activation -> max_pooling\n",
        "    x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "    x = x.view(batchsize,-1) # Change the shape of output from [batch_size, channels, height, width] to [batch_size, channels*height*width]\n",
        "    x = F.relu(self.bn3(self.fc1(x)))\n",
        "    out = self.fc2(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "t2VraVvGRcbE"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Train & Evaluation Code**\n",
        "Here is an example Train and Evaluation code for the baseline FC layer model. You will most likely have to change the code below to fit your model of choice."
      ],
      "metadata": {
        "id": "0CQzW4akNv9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataset, val_dataset, model, device='cuda:0' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True) #256\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False) #128\n",
        "    learning_rate = 0.01 #0.001\n",
        "    epoch_no = 3 #5 #10\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    print(\"====TRAIN INITIATED====\")\n",
        "    print(model)\n",
        "\n",
        "    for epoch in tqdm(range(epoch_no)):\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for idx, data in tqdm(enumerate(train_dataloader)):\n",
        "            imgs, labels = data\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            output = model(imgs)\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            #print(\"output\",output)\n",
        "            #print(labels)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            model_loss = F.cross_entropy(output, labels)\n",
        "            #print(\"output:\", output)\n",
        "            #print(\"labels:\", labels)\n",
        "            running_loss += model_loss.item()\n",
        "            model_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_dataloader)\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "        print(f\"Epoch {epoch + 1} - Average training loss: {avg_train_loss:.3f}, Training accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # validation loop every 3 epochs\n",
        "        if (epoch + 1) % 3 == 0:\n",
        "            val_loss = 0\n",
        "            correct_val = 0\n",
        "            total_val = 0\n",
        "            with torch.no_grad():\n",
        "                for val_idx, (val_imgs, val_labels) in enumerate(val_dataloader):\n",
        "                    val_imgs, val_labels = val_imgs.to(device), val_labels.to(device)\n",
        "                    val_output = model(val_imgs)\n",
        "\n",
        "                    _, val_predicted = torch.max(val_output.data, 1)\n",
        "                    total_val += val_labels.size(0)\n",
        "                    correct_val += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "                    val_loss += F.cross_entropy(val_output, val_labels)\n",
        "\n",
        "                val_accuracy = 100 * correct_val / total_val\n",
        "                print(f'Validation loss: {val_loss/(val_idx+1):.3f}, Validation accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "def test(test_dataset, model, device='cuda:0' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (test_imgs, test_labels) in enumerate(test_dataloader):\n",
        "            test_imgs, test_labels = test_imgs.to(device), test_labels.to(device)\n",
        "            test_output = model(test_imgs)\n",
        "\n",
        "            _, predicted = torch.max(test_output.data, 1)\n",
        "            total_test += test_labels.size(0)\n",
        "            correct_test += (predicted == test_labels).sum().item()\n",
        "\n",
        "            test_loss += F.cross_entropy(test_output, test_labels)\n",
        "\n",
        "        test_accuracy = 100 * correct_test / total_test\n",
        "        print(f'Test loss: {test_loss/(idx+1):.3f}, Test accuracy: {test_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "Um5qjYNhpWl-"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Run Code**\n",
        "\n",
        "Run the code below to run all the whole model."
      ],
      "metadata": {
        "id": "kNtYWN6WN6al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(1)"
      ],
      "metadata": {
        "id": "lNBF6vCIDlUL"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    dataset = Dataset(DATASET_PATH1)\n",
        "    dataset_size = len(dataset)\n",
        "    print(dataset_size)\n",
        "    train_size = int(dataset_size * 0.8)\n",
        "    validation_size = int(dataset_size * 0.1)\n",
        "    test_size = dataset_size - train_size - validation_size\n",
        "\n",
        "    train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "    print(f\"Training Data Size : {len(train_dataset)}\")\n",
        "    print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
        "    print(f\"Testing Data Size : {len(test_dataset)}\")\n",
        "\n",
        "    model_cnn_last = CNN_BN2()\n",
        "    train(train_dataset, validation_dataset, model_cnn_last)\n",
        "    test(test_dataset, model_cnn_last)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_KXck9BR4Bk",
        "outputId": "8b8b5107-61c0-450d-8476-9129bf9ba0d4"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2188\n",
            "Training Data Size : 1750\n",
            "Validation Data Size : 218\n",
            "Testing Data Size : 220\n",
            "====TRAIN INITIATED====\n",
            "CNN_BN2(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  1.23it/s]\u001b[A\n",
            "2it [00:01,  1.19it/s]\u001b[A\n",
            "3it [00:02,  1.21it/s]\u001b[A\n",
            "4it [00:03,  1.19it/s]\u001b[A\n",
            "5it [00:04,  1.23it/s]\u001b[A\n",
            "6it [00:04,  1.20it/s]\u001b[A\n",
            "7it [00:05,  1.24it/s]\n",
            " 33%|███▎      | 1/3 [00:05<00:11,  5.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average training loss: 0.480, Training accuracy: 81.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  1.38it/s]\u001b[A\n",
            "2it [00:01,  1.36it/s]\u001b[A\n",
            "3it [00:02,  1.30it/s]\u001b[A\n",
            "4it [00:03,  1.28it/s]\u001b[A\n",
            "5it [00:03,  1.29it/s]\u001b[A\n",
            "6it [00:04,  1.30it/s]\u001b[A\n",
            "7it [00:05,  1.32it/s]\n",
            " 67%|██████▋   | 2/3 [00:10<00:05,  5.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Average training loss: 0.126, Training accuracy: 96.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  1.18it/s]\u001b[A\n",
            "2it [00:01,  1.23it/s]\u001b[A\n",
            "3it [00:02,  1.25it/s]\u001b[A\n",
            "4it [00:03,  1.24it/s]\u001b[A\n",
            "5it [00:04,  1.23it/s]\u001b[A\n",
            "6it [00:04,  1.22it/s]\u001b[A\n",
            "7it [00:05,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Average training loss: 0.061, Training accuracy: 98.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:17<00:00,  5.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.035, Validation accuracy: 98.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.285, Test accuracy: 91.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    dataset1 = Dataset(DATASET_PATH1)\n",
        "    use_dataset1, unuse_dataset1 = random_split(dataset1, [0.54, 0.46])\n",
        "    dataset2 = Dataset(DATASET_PATH2)\n",
        "    print(len(dataset2))\n",
        "    dataset = use_dataset1 + dataset2\n",
        "    dataset_size = len(dataset)\n",
        "    print(dataset_size)\n",
        "    train_size = int(dataset_size * 0.8)\n",
        "    validation_size = int(dataset_size * 0.1)\n",
        "    test_size = dataset_size - train_size - validation_size\n",
        "\n",
        "\n",
        "    train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "    print(f\"Training Data Size : {len(train_dataset)}\")\n",
        "    print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
        "    print(f\"Testing Data Size : {len(test_dataset)}\")\n",
        "\n",
        "    train(train_dataset, validation_dataset, model_cnn_last)\n",
        "    test(test_dataset, model_cnn_last)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1amS8llGSVtv",
        "outputId": "b2b5b7ea-955b-4dde-dd96-c7799f80d71b"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2925\n",
            "4107\n",
            "Training Data Size : 3285\n",
            "Validation Data Size : 410\n",
            "Testing Data Size : 412\n",
            "====TRAIN INITIATED====\n",
            "CNN_BN2(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  1.18it/s]\u001b[A\n",
            "2it [00:01,  1.15it/s]\u001b[A\n",
            "3it [00:02,  1.12it/s]\u001b[A\n",
            "4it [00:03,  1.10it/s]\u001b[A\n",
            "5it [00:04,  1.11it/s]\u001b[A\n",
            "6it [00:05,  1.13it/s]\u001b[A\n",
            "7it [00:06,  1.10it/s]\u001b[A\n",
            "8it [00:07,  1.04it/s]\u001b[A\n",
            "9it [00:08,  1.05it/s]\u001b[A\n",
            "10it [00:09,  1.05it/s]\u001b[A\n",
            "11it [00:10,  1.08it/s]\u001b[A\n",
            "12it [00:11,  1.08it/s]\u001b[A\n",
            "13it [00:11,  1.10it/s]\n",
            " 33%|███▎      | 1/3 [00:11<00:23, 11.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average training loss: 0.750, Training accuracy: 77.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  1.14it/s]\u001b[A\n",
            "2it [00:01,  1.12it/s]\u001b[A\n",
            "3it [00:02,  1.10it/s]\u001b[A\n",
            "4it [00:03,  1.12it/s]\u001b[A\n",
            "5it [00:04,  1.13it/s]\u001b[A\n",
            "6it [00:05,  1.12it/s]\u001b[A\n",
            "7it [00:06,  1.13it/s]\u001b[A\n",
            "8it [00:07,  1.13it/s]\u001b[A\n",
            "9it [00:08,  1.10it/s]\u001b[A\n",
            "10it [00:09,  1.05it/s]\u001b[A\n",
            "11it [00:10,  1.05it/s]\u001b[A\n",
            "12it [00:11,  1.05it/s]\u001b[A\n",
            "13it [00:11,  1.11it/s]\n",
            " 67%|██████▋   | 2/3 [00:23<00:11, 11.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Average training loss: 0.202, Training accuracy: 95.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:00,  1.08it/s]\u001b[A\n",
            "2it [00:01,  1.11it/s]\u001b[A\n",
            "3it [00:02,  1.11it/s]\u001b[A\n",
            "4it [00:03,  1.14it/s]\u001b[A\n",
            "5it [00:04,  1.13it/s]\u001b[A\n",
            "6it [00:05,  1.14it/s]\u001b[A\n",
            "7it [00:06,  1.09it/s]\u001b[A\n",
            "8it [00:07,  1.07it/s]\u001b[A\n",
            "9it [00:08,  1.08it/s]\u001b[A\n",
            "10it [00:09,  1.07it/s]\u001b[A\n",
            "11it [00:10,  1.06it/s]\u001b[A\n",
            "12it [00:11,  1.05it/s]\u001b[A\n",
            "13it [00:11,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Average training loss: 0.102, Training accuracy: 97.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:37<00:00, 12.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.129, Validation accuracy: 98.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.084, Test accuracy: 97.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 전체 저장\n",
        "PATH = '/content/drive/MyDrive/MLintro/last.pth'\n",
        "torch.save(model_cnn_last, PATH)"
      ],
      "metadata": {
        "id": "Mo0hYh6N1WeJ"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn_last =  torch.load('/content/drive/MyDrive/MLintro/last.pth')"
      ],
      "metadata": {
        "id": "GNGEuIMt2m0y"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    dataset1 = Dataset(DATASET_PATH1)\n",
        "    use_dataset1, unuse_dataset1 = random_split(dataset1, [0.32, 1-0.32])\n",
        "    dataset2 = Dataset(DATASET_PATH2)\n",
        "    use_dataset2, unuse_dataset2 = random_split(dataset2, [0.24, 1-0.24])\n",
        "\n",
        "    dataset3 = Dataset(DATASET_PATH3)\n",
        "    dataset = use_dataset1 + use_dataset2 + dataset3\n",
        "    print(len(dataset3))\n",
        "    dataset_size = len(dataset)\n",
        "    print(dataset_size)\n",
        "    train_size = int(dataset_size * 0.8)\n",
        "    validation_size = int(dataset_size * 0.1)\n",
        "    test_size = dataset_size - train_size - validation_size\n",
        "\n",
        "\n",
        "    train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "    print(f\"Training Data Size : {len(train_dataset)}\")\n",
        "    print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
        "    print(f\"Testing Data Size : {len(test_dataset)}\")\n",
        "\n",
        "    train(train_dataset, validation_dataset, model_cnn_last)\n",
        "    test(test_dataset, model_cnn_last)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MUEDfsREXSZ",
        "outputId": "9b04d6f0-c3f5-4f2a-cafd-f467c6eada7a"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2746\n",
            "4149\n",
            "Training Data Size : 3319\n",
            "Validation Data Size : 414\n",
            "Testing Data Size : 416\n",
            "====TRAIN INITIATED====\n",
            "CNN_BN2(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [04:02, 242.27s/it]\u001b[A\n",
            "2it [07:37, 226.08s/it]\u001b[A\n",
            "3it [11:09, 219.75s/it]\u001b[A\n",
            "4it [14:13, 205.63s/it]\u001b[A\n",
            "5it [17:30, 202.62s/it]\u001b[A\n",
            "6it [20:53, 202.74s/it]\u001b[A\n",
            "7it [24:09, 200.67s/it]\u001b[A\n",
            "8it [27:22, 198.24s/it]\u001b[A\n",
            "9it [31:03, 205.34s/it]\u001b[A\n",
            "10it [34:36, 207.64s/it]\u001b[A\n",
            "11it [38:14, 210.73s/it]\u001b[A\n",
            "12it [42:08, 217.70s/it]\u001b[A\n",
            "13it [45:40, 210.80s/it]\n",
            " 33%|███▎      | 1/3 [45:40<1:31:20, 2740.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average training loss: 0.717, Training accuracy: 76.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:35, 35.21s/it]\u001b[A\n",
            "2it [01:08, 34.20s/it]\u001b[A\n",
            "3it [01:39, 32.81s/it]\u001b[A\n",
            "4it [02:12, 32.90s/it]\u001b[A\n",
            "5it [02:47, 33.69s/it]\u001b[A\n",
            "6it [03:21, 33.63s/it]\u001b[A\n",
            "7it [03:55, 33.88s/it]\u001b[A\n",
            "8it [04:31, 34.32s/it]\u001b[A\n",
            "9it [05:04, 33.98s/it]\u001b[A\n",
            "10it [05:38, 34.07s/it]\u001b[A\n",
            "11it [06:14, 34.52s/it]\u001b[A\n",
            "12it [06:51, 35.31s/it]\u001b[A\n",
            "13it [07:27, 34.41s/it]\n",
            " 67%|██████▋   | 2/3 [53:07<23:11, 1391.52s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Average training loss: 0.271, Training accuracy: 91.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:30, 30.82s/it]\u001b[A\n",
            "2it [01:06, 33.77s/it]\u001b[A\n",
            "3it [01:41, 34.30s/it]\u001b[A\n",
            "4it [02:15, 34.31s/it]\u001b[A\n",
            "5it [02:51, 34.72s/it]\u001b[A\n",
            "6it [03:24, 34.34s/it]\u001b[A\n",
            "7it [04:01, 35.14s/it]\u001b[A\n",
            "8it [04:36, 35.16s/it]\u001b[A\n",
            "9it [05:12, 35.22s/it]\u001b[A\n",
            "10it [05:46, 34.97s/it]\u001b[A\n",
            "11it [06:22, 35.33s/it]\u001b[A\n",
            "12it [06:53, 33.79s/it]\u001b[A\n",
            "13it [07:26, 34.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Average training loss: 0.103, Training accuracy: 97.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [1:07:19<00:00, 1346.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.081, Validation accuracy: 98.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.183, Test accuracy: 94.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 전체 저장\n",
        "PATH = '/content/drive/MyDrive/MLintro/last1.pth'\n",
        "torch.save(model_cnn_last, PATH)"
      ],
      "metadata": {
        "id": "88pXrjEzIrgI"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 전체 로드\n",
        "loaded_model = torch.load('/content/drive/MyDrive/MLintro/last1.pth')"
      ],
      "metadata": {
        "id": "BoVx4meuDOgt"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Inference**\n",
        "\n",
        "Once you are satisfied with your classifier code, you are to edit the code below so that it runs. The code below should successfully be able to load the 'test_image.jpeg' within the 'Project items' folder in the link given above and give a prediction for the image using your proposed model.\n",
        "\n",
        "The final output should return either 'rock', 'scissors' or 'paper'."
      ],
      "metadata": {
        "id": "6nt-yivD96LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = torch.load(PATH)"
      ],
      "metadata": {
        "id": "bly3g80fX_l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chosen_img_path = '/content/drive/My Drive/archive/inference\\test_image_jpeg'\n",
        "chosen_img_path = '/content/drive/MyDrive/MLintro/hand_test/test_image.jpeg'\n",
        "################ Edit Here ################\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "img = cv2.imread(chosen_img_path)\n",
        "img = img.astype(np.float32)/255.0\n",
        "img = cv2.resize(img, (28, 28))\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "input = transform(img)\n",
        "input = input.to(device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "input = input.unsqueeze(0)\n",
        "loaded_model.eval()\n",
        "################ Edit Here ################\n",
        "\n",
        "output = loaded_model(input)\n",
        "\n",
        "_, predicted_class = torch.max(output, 1)\n",
        "\n",
        "class_labels = ['rock', 'scissors', 'paper']\n",
        "predicted_label = class_labels[predicted_class.item()]\n",
        "print(predicted_label)\n"
      ],
      "metadata": {
        "id": "4EL-n8PK5LTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed019de3-3490-4d2b-b23f-926dc25ac1dc"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scissors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/MLintro/Kimgeonwoo_20200045.pth'\n",
        "torch.save(loaded_model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "fiXMM0oxd6tO"
      },
      "execution_count": 288,
      "outputs": []
    }
  ]
}